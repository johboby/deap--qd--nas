"""
å®Œæ•´åº”ç”¨åœºæ™¯ç¤ºä¾‹ (Complete Application Scenarios)
å±•ç¤ºQD-NASæ¡†æ¶çš„å®é™…åº”ç”¨
"""

import numpy as np
from src.nas import (
    # QD-NASæ ¸å¿ƒ
    QDNASOptimizer, create_default_qd_nas,

    # æœç´¢ç©ºé—´å’Œç‰¹å¾æå–
    Architecture, SearchSpace, StaticCharacterization,

    # é«˜çº§QDç®—æ³•
    create_cvt_map_elites,

    # åˆ†å¸ƒå¼è®¡ç®—
    create_evaluator, DistributedNASOptimizer, WorkerConfig,

    # åŸºå‡†æµ‹è¯•
    create_benchmark, BenchmarkRunner,

    # ç«¯åˆ°ç«¯NAS
    EndToEndNAS, NASConfig,

    # å·¥å…·
    LoggerManager, Timer, ProgressBar,
    CheckpointManager, MetricsTracker,
    set_random_seed,

    # é”™è¯¯å¤„ç†
    ErrorHandler, retry, safe_execute,
)

import logging


def scenario_1_mobile_nas():
    """
    åœºæ™¯1: ç§»åŠ¨ç«¯NASä¼˜åŒ–

    ç›®æ ‡ï¼šä¸ºç§»åŠ¨è®¾å¤‡æœç´¢ä½å»¶è¿Ÿã€ä½èƒ½è€—çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚
    """
    print("\n" + "="*70)
    print("åœºæ™¯1: ç§»åŠ¨ç«¯NASä¼˜åŒ–")
    print("="*70)

    # è®¾ç½®æ—¥å¿—
    logger_manager = LoggerManager(
        name='mobile_nas',
        level='INFO',
        log_file='./logs/mobile_nas.log'
    )
    logger = logger_manager.get_logger()

    # è®¾ç½®éšæœºç§å­
    set_random_seed(42)

    # åˆ›å»ºQD-NASä¼˜åŒ–å™¨ï¼ˆå¤šç›®æ ‡ï¼šå»¶è¿Ÿå’Œèƒ½è€—ï¼‰
    optimizer = create_default_qd_nas(
        optimization_mode='map_elites',
        multi_objective=True,
        population_guided=True
    )

    # é…ç½®çº¦æŸï¼ˆç§»åŠ¨è®¾å¤‡çº¦æŸï¼‰
    from src.nas import Constraint, ObjectiveType, Objective

    # å®šä¹‰ç§»åŠ¨è®¾å¤‡çš„å¤šç›®æ ‡
    objectives = [
        Objective(name='accuracy', type=ObjectiveType.MAXIMIZE, weight=0.5),
        Objective(name='latency', type=ObjectiveType.MINIMIZE, weight=0.3),
        Objective(name='energy', type=ObjectiveType.MINIMIZE, weight=0.2),
    ]

    constraints = [
        Constraint(name='latency', threshold=50.0, type="<="),  # 50ms
        Constraint(name='energy', threshold=500.0, type="<="),  # 500mJ
        Constraint(name='params', threshold=3.0, type="<="),  # 3Må‚æ•°
    ]

    # åˆå§‹åŒ–
    optimizer.initialize()

    # è¿è¡Œä¼˜åŒ–
    logger.info("ğŸš€ å¼€å§‹ç§»åŠ¨ç«¯NASä¼˜åŒ–")
    with Timer('Mobile NAS Optimization') as timer:
        archive, pareto_front = optimizer.optimize(
            n_iterations=200,
            batch_size=50,
            verbose=True
        )

    # åˆ†æç»“æœ
    logger.info(f"âœ… ä¼˜åŒ–å®Œæˆï¼Œè€—æ—¶: {timer.elapsed:.2f}s")

    print("\n" + "-"*70)
    print("ç§»åŠ¨ç«¯æ¶æ„æ¨è:")
    print("-"*70)

    # æ‰¾åˆ°æ»¡è¶³çº¦æŸçš„æœ€ä½³æ¶æ„
    for arch, metrics in pareto_front[:5]:
        latency_ok = metrics.latency <= 50
        energy_ok = metrics.energy <= 500
        params_ok = metrics.parameters / 1e6 <= 3

        print(f"\næ¶æ„ {pareto_front.index((arch, metrics)) + 1}:")
        print(f"  Accuracy: {metrics.accuracy:.4f}")
        print(f"  Latency: {metrics.latency:.2f}ms {'âœ“' if latency_ok else 'âœ—'}")
        print(f"  Energy: {metrics.energy:.2f}mJ {'âœ“' if energy_ok else 'âœ—'}")
        print(f"  Params: {metrics.parameters/1e6:.2f}M {'âœ“' if params_ok else 'âœ—'}")

        if latency_ok and energy_ok and params_ok:
            print(f"  âœ“âœ“âœ“ æ»¡è¶³æ‰€æœ‰ç§»åŠ¨è®¾å¤‡çº¦æŸï¼")


def scenario_2_distributed_nas():
    """
    åœºæ™¯2: åˆ†å¸ƒå¼NAS

    ç›®æ ‡ï¼šä½¿ç”¨å¤šè¿›ç¨‹å’ŒGPUåŠ é€Ÿè¿›è¡Œå¤§è§„æ¨¡NASæœç´¢ã€‚
    """
    print("\n" + "="*70)
    print("åœºæ™¯2: åˆ†å¸ƒå¼NAS")
    print("="*70)

    # è®¾ç½®æ—¥å¿—
    logger_manager = LoggerManager(
        name='distributed_nas',
        level='INFO',
        log_file='./logs/distributed_nas.log'
    )
    logger = logger_manager.get_logger()

    # åˆ›å»ºåˆ†å¸ƒå¼è¯„ä¼°å™¨é…ç½®
    worker_config = WorkerConfig(
        n_workers=4,  # ä½¿ç”¨4ä¸ªCPUæ ¸å¿ƒ
        use_gpu=False,  # å‡è®¾æ²¡æœ‰GPU
        max_tasks_per_worker=10
    )

    # åˆ›å»ºè¯„ä¼°å™¨
    from src.nas import MultiProcessEvaluator
    search_space = SearchSpace()

    # è¯„ä¼°å‡½æ•°
    def evaluate_architecture(arch):
        characterizer = StaticCharacterization()
        return characterizer.characterize(arch)

    evaluator = MultiProcessEvaluator(evaluate_architecture, worker_config)

    # åˆ›å»ºåŸºç¡€ä¼˜åŒ–å™¨
    base_optimizer = create_default_qd_nas(
        optimization_mode='map_elites',
        multi_objective=False,
        population_guided=True
    )

    # åˆ›å»ºåˆ†å¸ƒå¼NASä¼˜åŒ–å™¨
    distributed_optimizer = DistributedNASOptimizer(
        optimizer=base_optimizer,
        evaluator=evaluator,
        batch_size=50
    )

    # è¿è¡Œåˆ†å¸ƒå¼ä¼˜åŒ–
    logger.info("ğŸš€ å¼€å§‹åˆ†å¸ƒå¼NASä¼˜åŒ–")

    with Timer('Distributed NAS') as timer:
        archive, pareto_front = distributed_optimizer.optimize_distributed(
            n_iterations=100,
            verbose=True
        )

    logger.info(f"âœ… åˆ†å¸ƒå¼ä¼˜åŒ–å®Œæˆï¼Œè€—æ—¶: {timer.elapsed:.2f}s")

    # è¾“å‡ºç»“æœ
    print("\n" + "-"*70)
    print("åˆ†å¸ƒå¼NASç»“æœ:")
    print("-"*70)
    stats = archive.get_statistics()
    print(f"å½’æ¡£å¤§å°: {stats['size']}")
    print(f"è¦†ç›–ç‡: {stats['coverage']:.2%}")
    print(f"æœ€ä½³é€‚åº”åº¦: {stats['best_fitness']:.4f}")


def scenario_3_benchmark_comparison():
    """
    åœºæ™¯3: NASæ–¹æ³•åŸºå‡†æ¯”è¾ƒ

    ç›®æ ‡ï¼šæ¯”è¾ƒä¸åŒNASæ–¹æ³•çš„æ€§èƒ½ã€‚
    """
    print("\n" + "="*70)
    print("åœºæ™¯3: NASæ–¹æ³•åŸºå‡†æ¯”è¾ƒ")
    print("="*70)

    # è®¾ç½®æ—¥å¿—
    logger_manager = LoggerManager(
        name='benchmark_comparison',
        level='INFO',
        log_file='./logs/benchmark_comparison.log'
    )
    logger = logger_manager.get_logger()

    # åˆ›å»ºåŸºå‡†æµ‹è¯•
    benchmark = create_benchmark(dataset_name='cifar10')

    # åˆ›å»ºåŸºå‡†æµ‹è¯•è¿è¡Œå™¨
    search_space = SearchSpace()
    runner = BenchmarkRunner(benchmark, search_space)

    # æ¯”è¾ƒä¸åŒæ–¹æ³•
    methods = [
        'Random Search',
        'MAP-Elites',
        'CVT-MAP-Elites',
        'QD-NAS',
    ]

    results = {}
    for method in methods:
        logger.info(f"ğŸƒ è¿è¡ŒåŸºå‡†æµ‹è¯•: {method}")

        with Timer(f'{method} Benchmark') as timer:
            stats = runner.run_benchmark(method_name=method, n_samples=5)

        results[method] = stats
        logger.info(f"âœ… {method} å®Œæˆï¼Œè€—æ—¶: {timer.elapsed:.2f}s")

    # æ¯”è¾ƒç»“æœ
    print("\n" + "-"*70)
    print("åŸºå‡†æµ‹è¯•æ¯”è¾ƒ:")
    print("-"*70)

    print(f"\n{'æ–¹æ³•':<20} {'å¹³å‡å‡†ç¡®ç‡':<15} {'å¹³å‡å‚æ•°é‡':<15}")
    print("-"*70)

    for method, stats in results.items():
        params = stats['mean_parameters'] / 1e6
        print(f"{method:<20} {stats['mean_accuracy']:<15.4f} {params:<15.2f}M")

    # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
    best_method = max(results.keys(),
                   key=lambda k: results[k]['mean_accuracy'])

    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
    print(f"   å‡†ç¡®ç‡: {results[best_method]['mean_accuracy']:.4f}")


def scenario_4_robust_nas():
    """
    åœºæ™¯4: é²æ£’NASä¼˜åŒ–

    ç›®æ ‡ï¼šä½¿ç”¨é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶è¿›è¡Œé²æ£’çš„NASä¼˜åŒ–ã€‚
    """
    print("\n" + "="*70)
    print("åœºæ™¯4: é²æ£’NASä¼˜åŒ–")
    print("="*70)

    # è®¾ç½®æ—¥å¿—
    logger_manager = LoggerManager(
        name='robust_nas',
        level='INFO',
        log_file='./logs/robust_nas.log',
        log_file='./logs/robust_nas_errors.log'
    )
    logger = logger_manager.get_logger()

    # åˆ›å»ºé”™è¯¯å¤„ç†å™¨
    error_handler = ErrorHandler(
        error_log_file='./logs/robust_nas_errors.log',
        enable_recovery=True
    )

    # æ³¨å†Œæ¢å¤ç­–ç•¥
    from src.nas.error_handling import CheckpointRecoveryStrategy

    checkpoint_strategy = CheckpointRecoveryStrategy(checkpoint_dir='./checkpoints')
    error_handler.register_recovery_strategy(checkpoint_strategy)

    # åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpoint_manager = CheckpointManager(
        save_dir='./checkpoints',
        max_checkpoints=5
    )

    # åˆ›å»ºæŒ‡æ ‡è·Ÿè¸ªå™¨
    metrics_tracker = MetricsTracker(
        metrics_names=['accuracy', 'latency', 'energy', 'diversity']
    )

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_default_qd_nas(
        optimization_mode='map_elites',
        multi_objective=False,
        population_guided=True
    )

    # åˆå§‹åŒ–
    optimizer.initialize()

    # é²æ£’ä¼˜åŒ–å¾ªç¯
    logger.info("ğŸš€ å¼€å§‹é²æ£’NASä¼˜åŒ–")

    iteration = 0
    max_iterations = 100

    with Timer('Robust NAS') as timer:
        progress_bar = ProgressBar(total=max_iterations, desc='ä¼˜åŒ–è¿›åº¦')

        while iteration < max_iterations:
            try:
                # æ­£å¸¸ä¼˜åŒ–æ­¥éª¤
                archive, pareto_front = optimizer.optimize(
                    n_iterations=1,
                    batch_size=20,
                    verbose=False
                )

                # æ›´æ–°æŒ‡æ ‡
                stats = archive.get_statistics()
                metrics_tracker.update(
                    step=iteration,
                    accuracy=stats['best_fitness'],
                    latency=stats.get('latency', 0),
                    energy=stats.get('energy', 0),
                    diversity=stats['diversity']
                )

                # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯10è½®ï¼‰
                if iteration % 10 == 0:
                    checkpoint_data = {
                        'iteration': iteration,
                        'archive': archive,
                        'metrics': stats,
                    }
                    checkpoint_manager.save_checkpoint(
                        data=checkpoint_data,
                        epoch=iteration
                    )

                iteration += 1
                progress_bar.update()

            except Exception as e:
                logger.error(f"âŒ è¿­ä»£ {iteration} å¤±è´¥: {e}")

                # å°è¯•é”™è¯¯æ¢å¤
                context = {
                    'iteration': iteration,
                    'optimizer': 'QD-NAS',
                }

                if error_handler.handle_error(e, context):
                    # æ¢å¤æˆåŠŸï¼Œç»§ç»­
                    logger.info("âœ… é”™è¯¯æ¢å¤æˆåŠŸï¼Œç»§ç»­ä¼˜åŒ–")
                else:
                    # æ¢å¤å¤±è´¥ï¼Œå°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤
                    logger.warning("âš ï¸  é”™è¯¯æ¢å¤å¤±è´¥ï¼Œå°è¯•ä»æ£€æŸ¥ç‚¹æ¢å¤")

                    latest_checkpoint = checkpoint_manager.get_latest_checkpoint()
                    if latest_checkpoint:
                        logger.info(f"ğŸ“¥ ä»æ£€æŸ¥ç‚¹æ¢å¤: {latest_checkpoint}")
                        # è¿™é‡Œåº”è¯¥å®é™…åŠ è½½æ£€æŸ¥ç‚¹
                        iteration += 1
                    else:
                        logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„æ£€æŸ¥ç‚¹ï¼Œåœæ­¢ä¼˜åŒ–")
                        break

        progress_bar.close()

    logger.info(f"âœ… é²æ£’NASä¼˜åŒ–å®Œæˆï¼Œè€—æ—¶: {timer.elapsed:.2f}s")

    # è¾“å‡ºé”™è¯¯ç»Ÿè®¡
    error_stats = error_handler.get_error_statistics()
    print("\n" + "-"*70)
    print("é”™è¯¯ç»Ÿè®¡:")
    print("-"*70)
    print(f"æ€»é”™è¯¯æ•°: {error_stats['total_errors']}")
    print(f"é”™è¯¯ç±»å‹åˆ†å¸ƒ: {error_stats['error_counts']}")

    # ä¿å­˜æŒ‡æ ‡
    metrics_tracker.save_to_csv('./results/robust_nas_metrics.csv')

    # è¾“å‡ºæœ€ç»ˆç»“æœ
    stats = optimizer.get_statistics()
    print(f"\næœ€ç»ˆç»Ÿè®¡:")
    print(f"å½’æ¡£å¤§å°: {stats['size']}")
    print(f"è¦†ç›–ç‡: {stats['coverage']:.2%}")
    print(f"æœ€ä½³é€‚åº”åº¦: {stats['best_fitness']:.4f}")


def scenario_5_end_to_end_nas():
    """
    åœºæ™¯5: ç«¯åˆ°ç«¯NAS

    ç›®æ ‡ï¼šå®Œæ•´çš„NASæµç¨‹ï¼Œä»æœç´¢åˆ°éƒ¨ç½²ã€‚
    """
    print("\n" + "="*70)
    print("åœºæ™¯5: ç«¯åˆ°ç«¯NAS")
    print("="*70)

    # è®¾ç½®æ—¥å¿—
    logger_manager = LoggerManager(
        name='end_to_end_nas',
        level='INFO',
        log_file='./logs/end_to_end_nas.log'
    )
    logger = logger_manager.get_logger()

    # åˆ›å»ºNASé…ç½®
    config = NASConfig(
        name='End-to-End NAS',
        description='å®Œæ•´çš„ç«¯åˆ°ç«¯ç¥ç»æ¶æ„æœç´¢',
        dataset='cifar10',
        optimization_mode='map_elites',
        multi_objective=True,
        population_guided=True,
        n_iterations=100,
        batch_size=50,
        epochs=50,
        early_stopping=True,
        patience=5,
        save_dir='./results/end_to_end',
        device='cpu',
    )

    # ä¿å­˜é…ç½®
    config.save('./results/end_to_end/config.json')

    # åˆ›å»ºç«¯åˆ°ç«¯NAS
    nas = EndToEndNAS(config)

    # è¿è¡Œç«¯åˆ°ç«¯NAS
    logger.info("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯NAS")

    with Timer('End-to-End NAS') as timer:
        result = nas.run()

    logger.info(f"âœ… ç«¯åˆ°ç«¯NASå®Œæˆï¼Œè€—æ—¶: {timer.elapsed:.2f}s")

    # ç»“æœå·²é€šè¿‡resultå¯¹è±¡è‡ªåŠ¨ä¿å­˜
    print(f"\nç»“æœå·²ä¿å­˜è‡³: {config.save_dir}")
    print(f"æŠ¥å‘Šæ–‡ä»¶: {config.save_dir}/report.txt")


def scenario_6_multi_objective_tradeoff():
    """
    åœºæ™¯6: å¤šç›®æ ‡æƒè¡¡åˆ†æ

    ç›®æ ‡ï¼šåˆ†æParetoå‰æ²¿ä¸Šçš„ä¸åŒæ¶æ„çš„æƒè¡¡ã€‚
    """
    print("\n" + "="*70)
    print("åœºæ™¯6: å¤šç›®æ ‡æƒè¡¡åˆ†æ")
    print("="*70)

    # è®¾ç½®æ—¥å¿—
    logger_manager = LoggerManager(
        name='multi_objective_tradeoff',
        level='INFO',
        log_file='./logs/multi_objective_tradeoff.log'
    )
    logger = logger_manager.get_logger()

    # åˆ›å»ºå¤šç›®æ ‡ä¼˜åŒ–å™¨
    optimizer = create_default_qd_nas(
        optimization_mode='map_elites',
        multi_objective=True,
        population_guided=True
    )

    # åˆå§‹åŒ–å’Œä¼˜åŒ–
    optimizer.initialize()

    logger.info("ğŸš€ å¼€å§‹å¤šç›®æ ‡ä¼˜åŒ–")
    archive, pareto_front = optimizer.optimize(
        n_iterations=100,
        batch_size=50,
        verbose=True
    )

    # åˆ†æParetoå‰æ²¿
    print("\n" + "-"*70)
    print("Paretoå‰æ²¿æƒè¡¡åˆ†æ:")
    print("-"*70)

    if not pareto_front:
        print("æ²¡æœ‰æ‰¾åˆ°Paretoå‰æ²¿")
        return

    # åˆ†ç±»æ¶æ„
    high_accuracy = []
    low_latency = []
    low_energy = []

    for arch, metrics in pareto_front:
        if metrics.accuracy > 0.85:
            high_accuracy.append((arch, metrics))
        if metrics.latency < 30:
            low_latency.append((arch, metrics))
        if metrics.energy < 300:
            low_energy.append((arch, metrics))

    print(f"\né«˜å‡†ç¡®ç‡æ¶æ„ (>85%): {len(high_accuracy)}")
    for i, (arch, metrics) in enumerate(high_accuracy[:3]):
        print(f"  {i+1}. Accuracy={metrics.accuracy:.4f}, "
              f"Latency={metrics.latency:.2f}ms, Energy={metrics.energy:.2f}mJ")

    print(f"\nä½å»¶è¿Ÿæ¶æ„ (<30ms): {len(low_latency)}")
    for i, (arch, metrics) in enumerate(low_latency[:3]):
        print(f"  {i+1}. Accuracy={metrics.accuracy:.4f}, "
              f"Latency={metrics.latency:.2f}ms, Energy={metrics.energy:.2f}mJ")

    print(f"\nä½èƒ½è€—æ¶æ„ (<300mJ): {len(low_energy)}")
    for i, (arch, metrics) in enumerate(low_energy[:3]):
        print(f"  {i+1}. Accuracy={metrics.accuracy:.4f}, "
              f"Latency={metrics.latency:.2f}ms, Energy={metrics.energy:.2f}mJ")

    # æ‰¾åˆ°æƒè¡¡æœ€ä¼˜çš„æ¶æ„ï¼ˆç»¼åˆå¾—åˆ†ï¼‰
    best_arch, best_metrics = pareto_front[0]
    best_score = 0

    for arch, metrics in pareto_front:
        # ç»¼åˆè¯„åˆ†ï¼ˆæƒé‡å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
        score = 0.5 * metrics.accuracy + \
                0.25 * (1 - metrics.latency / 100) + \
                0.25 * (1 - metrics.energy / 1000)

        if score > best_score:
            best_score = score
            best_arch = arch
            best_metrics = metrics

    print(f"\nğŸ† æœ€ä¼˜æƒè¡¡æ¶æ„:")
    print(f"  Accuracy: {best_metrics.accuracy:.4f}")
    print(f"  Latency: {best_metrics.latency:.2f}ms")
    print(f"  Energy: {best_metrics.energy:.2f}mJ")
    print(f"  Parameters: {best_metrics.parameters/1e6:.2f}M")
    print(f"  ç»¼åˆå¾—åˆ†: {best_score:.4f}")


def main():
    """è¿è¡Œæ‰€æœ‰åœºæ™¯"""
    print("\n" + "="*70)
    print("QD-NAS å®Œæ•´åº”ç”¨åœºæ™¯ç¤ºä¾‹")
    print("="*70)

    # è¿è¡Œæ‰€æœ‰åœºæ™¯
    scenario_1_mobile_nas()
    scenario_2_distributed_nas()
    scenario_3_benchmark_comparison()
    scenario_4_robust_nas()
    scenario_5_end_to_end_nas()
    scenario_6_multi_objective_tradeoff()

    print("\n" + "="*70)
    print("æ‰€æœ‰åœºæ™¯è¿è¡Œå®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
