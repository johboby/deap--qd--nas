"""
ç«¯åˆ°ç«¯NASæµç¨‹ (End-to-End NAS Pipeline)
å®Œæ•´çš„æ•°æ®åŠ è½½ã€è®­ç»ƒã€è¯„ä¼°ã€å¯¼å‡ºæµç¨‹
"""

import numpy as np
from typing import List, Dict, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import logging
import time
import json
import os
from pathlib import Path

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

from .qd_nas import QDNASOptimizer
from .search_space import Architecture
from .characterization import ArchitectureMetrics
from .benchmark_suite import StandardDatasets
from .distributed_computing import create_evaluator, BaseEvaluator


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class NASConfig:
    """
    NASé…ç½®

    Args:
        optimization_mode: ä¼˜åŒ–æ¨¡å¼
        multi_objective: æ˜¯å¦å¤šç›®æ ‡ä¼˜åŒ–
        population_guided: æ˜¯å¦ç§ç¾¤å¼•å¯¼æœç´¢
        n_iterations: è¿­ä»£æ¬¡æ•°
        batch_size: æ‰¹å¤„ç†å¤§å°
        dataset: æ•°æ®é›†åç§°
        epochs: è®­ç»ƒè½®æ•°
        early_stopping: æ˜¯å¦æ—©åœ
        patience: æ—©åœè€å¿ƒå€¼
        save_dir: ä¿å­˜ç›®å½•
        device: è®¡ç®—è®¾å¤‡
    """
    optimization_mode: str = 'map_elites'
    multi_objective: bool = False
    population_guided: bool = True
    n_iterations: int = 1000
    batch_size: int = 100
    dataset: str = 'cifar10'
    epochs: int = 50
    early_stopping: bool = True
    patience: int = 5
    save_dir: str = './nas_results'
    device: str = 'cpu'

    def save(self, filepath: str):
        """ä¿å­˜é…ç½®"""
        with open(filepath, 'w') as f:
            json.dump(self.__dict__, f, indent=2)
        logger.info(f"é…ç½®ä¿å­˜è‡³: {filepath}")

    @classmethod
    def load(cls, filepath: str) -> 'NASConfig':
        """åŠ è½½é…ç½®"""
        with open(filepath, 'r') as f:
            data = json.load(f)
        return cls(**data)


@dataclass
class NASResult:
    """
    NASç»“æœ

    å­˜å‚¨å®Œæ•´çš„NASæœç´¢ç»“æœã€‚
    """
    best_architecture: Optional[Architecture] = None
    best_metrics: Optional[ArchitectureMetrics] = None
    pareto_front: List[Tuple[Architecture, ArchitectureMetrics]] = field(default_factory=list)
    archive_statistics: Dict[str, Any] = field(default_factory=dict)
    optimization_history: List[Dict[str, Any]] = field(default_factory=list)
    total_time: float = 0.0
    config: Optional[NASConfig] = None

    def save(self, save_dir: str):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(save_dir, exist_ok=True)

        # ä¿å­˜æœ€ä½³æ¶æ„
        if self.best_architecture:
            with open(f'{save_dir}/best_architecture.json', 'w') as f:
                json.dump(self.best_architecture.to_dict(), f, indent=2)

        # ä¿å­˜Paretoå‰æ²¿
        if self.pareto_front:
            pareto_data = []
            for arch, metrics in self.pareto_front:
                pareto_data.append({
                    'architecture': arch.to_dict(),
                    'metrics': metrics.to_dict(),
                })
            with open(f'{save_dir}/pareto_front.json', 'w') as f:
                json.dump(pareto_data, f, indent=2)

        # ä¿å­˜å†å²
        with open(f'{save_dir}/optimization_history.json', 'w') as f:
            json.dump(self.optimization_history, f, indent=2)

        # ä¿å­˜ç»Ÿè®¡
        with open(f'{save_dir}/statistics.json', 'w') as f:
            json.dump(self.archive_statistics, f, indent=2)

        # ä¿å­˜é…ç½®
        if self.config:
            self.config.save(f'{save_dir}/config.json')

        logger.info(f"ç»“æœä¿å­˜è‡³: {save_dir}")

    def generate_report(self) -> str:
        """ç”ŸæˆæŠ¥å‘Š"""
        report = []
        report.append("="*60)
        report.append("NASä¼˜åŒ–æŠ¥å‘Š")
        report.append("="*60)

        if self.best_architecture and self.best_metrics:
            report.append("\næœ€ä½³æ¶æ„:")
            report.append(f"  å‡†ç¡®ç‡: {self.best_metrics.accuracy:.4f}")
            report.append(f"  å»¶è¿Ÿ: {self.best_metrics.latency:.2f}ms")
            report.append(f"  èƒ½è€—: {self.best_metrics.energy:.2f}mJ")
            report.append(f"  å‚æ•°é‡: {self.best_metrics.parameters/1e6:.2f}M")
            report.append(f"  è®¡ç®—é‡: {self.best_metrics.flops/1e6:.2f}M")

        if self.pareto_front:
            report.append(f"\nParetoå‰æ²¿å¤§å°: {len(self.pareto_front)}")

        if self.archive_statistics:
            report.append("\nå½’æ¡£ç»Ÿè®¡:")
            report.append(f"  å½’æ¡£å¤§å°: {self.archive_statistics.get('size', 0)}")
            report.append(f"  è¦†ç›–ç‡: {self.archive_statistics.get('coverage', 0):.2%}")
            report.append(f"  å¤šæ ·æ€§: {self.archive_statistics.get('diversity', 0):.4f}")

        report.append(f"\næ€»è¿è¡Œæ—¶é—´: {self.total_time:.2f}s")
        report.append("="*60)

        return "\n".join(report)


class DataPipeline:
    """
    æ•°æ®ç®¡é“

    å¤„ç†æ•°æ®åŠ è½½ã€é¢„å¤„ç†å’Œå¢å¼ºã€‚
    """

    def __init__(self, dataset_config, batch_size: int = 128):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡é“

        Args:
            dataset_config: æ•°æ®é›†é…ç½®
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.dataset_config = dataset_config
        self.batch_size = batch_size

        self._load_dataset()

    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch is required for data pipeline")

        import torchvision
        import torchvision.transforms as transforms

        logger.info(f"ğŸ“¥ åŠ è½½æ•°æ®é›†: {self.dataset_config.name}")

        # æ•°æ®å¢å¼º
        transform_train = transforms.Compose([
            transforms.RandomCrop(self.dataset_config.input_shape[1], padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(self.dataset_config.mean, self.dataset_config.std),
        ])

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(self.dataset_config.mean, self.dataset_config.std),
        ])

        # åŠ è½½æ•°æ®é›†
        if self.dataset_config.name.lower() == 'cifar10':
            self.trainset = torchvision.datasets.CIFAR10(
                root='./data', train=True, download=True, transform=transform_train
            )
            self.testset = torchvision.datasets.CIFAR10(
                root='./data', train=False, download=True, transform=transform_test
            )
        elif self.dataset_config.name.lower() == 'cifar100':
            self.trainset = torchvision.datasets.CIFAR100(
                root='./data', train=True, download=True, transform=transform_train
            )
            self.testset = torchvision.datasets.CIFAR100(
                root='./data', train=False, download=True, transform=transform_test
            )
        else:
            raise ValueError(f"Unknown dataset: {self.dataset_config.name}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.trainloader = torch.utils.data.DataLoader(
            self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=2
        )
        self.testloader = torch.utils.data.DataLoader(
            self.testset, batch_size=self.batch_size, shuffle=False, num_workers=2
        )

        logger.info("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")

    def get_train_loader(self):
        """è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        return self.trainloader

    def get_test_loader(self):
        """è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        return self.testloader


class Trainer:
    """
    è®­ç»ƒå™¨

    è´Ÿè´£æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚
    """

    def __init__(self, model: nn.Module, device: str = 'cpu'):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            model: æ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.device = device

        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = None
        self.scheduler = None

        self.history = {
            'train_loss': [],
            'train_acc': [],
            'test_loss': [],
            'test_acc': [],
        }

    def setup_optimizer(self, lr: float = 0.1, momentum: float = 0.9, weight_decay: float = 5e-4):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        self.optimizer = optim.SGD(
            self.model.parameters(),
            lr=lr,
            momentum=momentum,
            weight_decay=weight_decay
        )

    def setup_scheduler(self, epochs: int):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, epochs)

    def train_epoch(self, trainloader) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in trainloader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, labels)
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        epoch_loss = running_loss / len(trainloader)
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc

    def test(self, testloader) -> Tuple[float, float]:
        """æµ‹è¯•æ¨¡å‹"""
        self.model.eval()
        test_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)

                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        avg_loss = test_loss / len(testloader)
        avg_acc = 100. * correct / total

        return avg_loss, avg_acc

    def train(self,
              trainloader,
              testloader,
              epochs: int = 50,
              verbose: bool = True) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            trainloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            testloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            epochs: è®­ç»ƒè½®æ•°
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯

        Returns:
            è®­ç»ƒå†å²
        """
        self.setup_scheduler(epochs)

        best_acc = 0.0
        patience_counter = 0

        for epoch in range(epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(trainloader)

            # æµ‹è¯•
            test_loss, test_acc = self.test(testloader)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['test_loss'].append(test_loss)
            self.history['test_acc'].append(test_acc)

            # æ—©åœæ£€æŸ¥
            if test_acc > best_acc:
                best_acc = test_acc
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                patience_counter += 1

            if verbose and (epoch + 1) % 5 == 0:
                logger.info(
                    f"Epoch {epoch + 1}/{epochs} | "
                    f"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | "
                    f"Test Loss: {test_loss:.4f} Acc: {test_acc:.2f}% | "
                    f"Best: {best_acc:.2f}%"
                )

            if patience_counter >= 5:
                logger.info(f"Early stopping at epoch {epoch + 1}")
                break

        return self.history


class EndToEndNAS:
    """
    ç«¯åˆ°ç«¯NAS

    å®Œæ•´çš„NASæœç´¢æµç¨‹ã€‚
    """

    def __init__(self, config: NASConfig):
        """
        åˆå§‹åŒ–ç«¯åˆ°ç«¯NAS

        Args:
            config: NASé…ç½®
        """
        self.config = config

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(config.save_dir, exist_ok=True)

        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = QDNASOptimizer(
            optimization_mode=config.optimization_mode,
            multi_objective=config.multi_objective,
            population_guided=config.population_guided
        )

        # åˆå§‹åŒ–æ•°æ®ç®¡é“
        dataset_config = StandardDatasets.get_cifar10()
        if config.dataset.lower() == 'cifar100':
            dataset_config = StandardDatasets.get_cifar100()
        elif config.dataset.lower() == 'mnist':
            dataset_config = StandardDatasets.get_mnist()

        self.data_pipeline = DataPipeline(
            dataset_config=dataset_config,
            batch_size=config.batch_size
        )

        # åˆ›å»ºè¯„ä¼°å™¨
        self.evaluator = create_evaluator(
            evaluate_function=self._evaluate_architecture,
            use_multiprocessing=True,
            n_workers=4
        )

        logger.info("ğŸš€ ç«¯åˆ°ç«¯NASåˆå§‹åŒ–å®Œæˆ")

    def _evaluate_architecture(self, architecture: Architecture) -> ArchitectureMetrics:
        """è¯„ä¼°æ¶æ„"""
        # åˆ›å»ºæ¨¡å‹
        model = self._create_model(architecture)

        # è®­ç»ƒ
        trainer = Trainer(model, device=self.config.device)
        trainer.setup_optimizer()
        history = trainer.train(
            self.data_pipeline.get_train_loader(),
            self.data_pipeline.get_test_loader(),
            epochs=self.config.epochs,
            verbose=False
        )

        # æµ‹è¯•
        test_acc = np.max(history['test_acc'])

        # ä¼°è®¡æ€§èƒ½æŒ‡æ ‡
        n_params = sum(p.numel() for p in model.parameters())
        latency = n_params / 1e6 * 10
        energy = n_params / 1e6 * 5

        metrics = ArchitectureMetrics(
            accuracy=test_acc / 100,
            latency=latency,
            energy=energy,
            parameters=n_params,
            flops=n_params * 10,
            depth=architecture.n_cells,
            width=architecture.n_channels,
            memory=n_params * 4 / (1024 ** 2),
            operation_diversity=0.8,
            skip_connections=0,
        )

        return metrics

    def _create_model(self, architecture: Architecture) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        class SimpleCNN(nn.Module):
            def __init__(self, arch):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, arch.n_channels, 3, padding=1),
                    nn.BatchNorm2d(arch.n_channels),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(arch.n_channels, arch.n_channels * 2, 3, padding=1),
                    nn.BatchNorm2d(arch.n_channels * 2),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                )
                self.classifier = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(arch.n_channels * 2 * 8 * 8, 256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(256, 10),
                )

            def forward(self, x):
                x = self.features(x)
                x = self.classifier(x)
                return x

        model = SimpleCNN(architecture).to(self.config.device)
        return model

    def run(self) -> NASResult:
        """
        è¿è¡Œç«¯åˆ°ç«¯NAS

        Returns:
            NASç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯NASæœç´¢")

        start_time = time.time()

        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer.initialize()

        # è¿è¡Œä¼˜åŒ–
        archive, pareto_front = self.optimizer.optimize(
            n_iterations=self.config.n_iterations,
            batch_size=self.config.batch_size,
            verbose=True
        )

        # è·å–æœ€ä½³æ¶æ„
        best_arch = self.optimizer.get_best_architecture()
        best_metrics = None
        if best_arch:
            best_metrics = self.evaluator.evaluate([best_arch])[0]

        # è®°å½•å†å²
        history = []
        for i, stats in enumerate(archive.get('history', [])):
            history.append({
                'iteration': i,
                'size': stats.get('size', 0),
                'coverage': stats.get('coverage', 0),
                'best_fitness': stats.get('best_fitness', 0),
            })

        # åˆ›å»ºç»“æœ
        result = NASResult(
            best_architecture=best_arch,
            best_metrics=best_metrics,
            pareto_front=pareto_front,
            archive_statistics=archive.get_statistics(),
            optimization_history=history,
            total_time=time.time() - start_time,
            config=self.config
        )

        # ä¿å­˜ç»“æœ
        result.save(self.config.save_dir)

        # ç”ŸæˆæŠ¥å‘Š
        report = result.generate_report()
        print(report)

        # ä¿å­˜æŠ¥å‘Š
        with open(f'{self.config.save_dir}/report.txt', 'w') as f:
            f.write(report)

        logger.info(f"âœ… ç«¯åˆ°ç«¯NASå®Œæˆï¼Œè€—æ—¶: {result.total_time:.2f}s")

        return result


def create_end_to_end_nas(config: NASConfig) -> EndToEndNAS:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºç«¯åˆ°ç«¯NAS

    Args:
        config: NASé…ç½®

    Returns:
        ç«¯åˆ°ç«¯NASå¯¹è±¡
    """
    return EndToEndNAS(config)


__all__ = [
    'NASConfig',
    'NASResult',
    'DataPipeline',
    'Trainer',
    'EndToEndNAS',
    'create_end_to_end_nas',
]
