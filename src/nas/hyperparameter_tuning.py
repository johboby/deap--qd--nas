"""
è¶…å‚æ•°è°ƒä¼˜æ¨¡å— (Hyperparameter Tuning)
å®ç°è‡ªé€‚åº”å‚æ•°è°ƒæ•´å’Œè´å¶æ–¯ä¼˜åŒ–
"""

import numpy as np
from typing import List, Dict, Any, Optional, Callable, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging

try:
    from scipy.optimize import minimize
    from scipy.stats import norm
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    from sklearn.gaussian_process import GaussianProcessRegressor
    from sklearn.gaussian_process.kernels import RBF, ConstantKernel
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class Hyperparameter:
    """
    è¶…å‚æ•°å®šä¹‰

    Args:
        name: å‚æ•°åç§°
        type: å‚æ•°ç±»å‹ ('continuous', 'discrete', 'categorical')
        min_val: æœ€å°å€¼ï¼ˆè¿ç»­/ç¦»æ•£ï¼‰
        max_val: æœ€å¤§å€¼ï¼ˆè¿ç»­/ç¦»æ•£ï¼‰
        choices: å¯é€‰å€¼ï¼ˆç±»åˆ«å‹ï¼‰
        default: é»˜è®¤å€¼
    """
    name: str
    type: str  # 'continuous', 'discrete', 'categorical'
    min_val: Optional[float] = None
    max_val: Optional[float] = None
    choices: Optional[List[Any]] = None
    default: Optional[Any] = None

    def __post_init__(self):
        """éªŒè¯å‚æ•°"""
        if self.type not in ['continuous', 'discrete', 'categorical']:
            raise ValueError(f"Invalid type: {self.type}")

        if self.type == 'categorical':
            if self.choices is None:
                raise ValueError("Categorical parameter must have choices")
        elif self.type in ['continuous', 'discrete']:
            if self.min_val is None or self.max_val is None:
                raise ValueError("Continuous/discrete parameter must have min_val and max_val")

    def sample(self) -> Any:
        """é‡‡æ ·ä¸€ä¸ªå€¼"""
        if self.type == 'continuous':
            return np.random.uniform(self.min_val, self.max_val)
        elif self.type == 'discrete':
            return np.random.randint(int(self.min_val), int(self.max_val) + 1)
        elif self.type == 'categorical':
            return np.random.choice(self.choices)
        else:
            raise ValueError(f"Unknown type: {self.type}")


class BaseHyperparameterOptimizer(ABC):
    """
    è¶…å‚æ•°ä¼˜åŒ–å™¨åŸºç±»
    """

    def __init__(self,
                 hyperparameters: List[Hyperparameter],
                 objective_function: Callable[[Dict[str, Any]], float]):
        """
        åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨

        Args:
            hyperparameters: è¶…å‚æ•°åˆ—è¡¨
            objective_function: ç›®æ ‡å‡½æ•°
        """
        self.hyperparameters = hyperparameters
        self.objective_function = objective_function

        # ä¼˜åŒ–å†å²
        self.history = []

    @abstractmethod
    def optimize(self, n_iterations: int = 100) -> Tuple[Dict[str, Any], float]:
        """
        ä¼˜åŒ–è¶…å‚æ•°

        Args:
            n_iterations: è¿­ä»£æ¬¡æ•°

        Returns:
            (best_params, best_score)
        """
        pass


class RandomSearchOptimizer(BaseHyperparameterOptimizer):
    """
    éšæœºæœç´¢ä¼˜åŒ–å™¨

    ç®€å•çš„éšæœºæœç´¢åŸºçº¿æ–¹æ³•ã€‚
    """

    def optimize(self, n_iterations: int = 100) -> Tuple[Dict[str, Any], float]:
        """
        ä¼˜åŒ–è¶…å‚æ•°

        Args:
            n_iterations: è¿­ä»£æ¬¡æ•°

        Returns:
            (best_params, best_score)
        """
        logger.info(f"ğŸ² å¼€å§‹éšæœºæœç´¢ä¼˜åŒ–ï¼Œè¿­ä»£æ¬¡æ•°: {n_iterations}")

        best_params = None
        best_score = -np.inf

        for iteration in range(n_iterations):
            # é‡‡æ ·å‚æ•°
            params = {hp.name: hp.sample() for hp in self.hyperparameters}

            # è¯„ä¼°
            score = self.objective_function(params)
            self.history.append((params, score))

            # æ›´æ–°æœ€ä½³
            if score > best_score:
                best_score = score
                best_params = params

            # è¾“å‡ºè¿›åº¦
            if (iteration + 1) % 10 == 0:
                logger.info(f"Iteration {iteration + 1}/{n_iterations} | Best: {best_score:.6f}")

        logger.info("âœ… éšæœºæœç´¢ä¼˜åŒ–å®Œæˆ")
        return best_params, best_score


class AdaptiveParameterTuner:
    """
    è‡ªé€‚åº”å‚æ•°è°ƒæ•´å™¨

    åŸºäºä¼˜åŒ–è¿‡ç¨‹ä¸­çš„åé¦ˆè‡ªåŠ¨è°ƒæ•´å‚æ•°ã€‚

    æ ¸å¿ƒç‰¹æ€§:
    1. å¤šæ ·æ€§è‡ªé€‚åº”å˜å¼‚ç‡
    2. æ€§èƒ½è‡ªé€‚åº”é€‰æ‹©å‹åŠ›
    3. å­¦ä¹ ç‡è‡ªé€‚åº”è°ƒæ•´
    4. æ‰¹æ¬¡å¤§å°è‡ªé€‚åº”
    """

    def __init__(self,
                 initial_params: Dict[str, float],
                 adaptation_rate: float = 0.1):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”å‚æ•°è°ƒæ•´å™¨

        Args:
            initial_params: åˆå§‹å‚æ•°
            adaptation_rate: è°ƒæ•´é€Ÿç‡
        """
        self.params = initial_params.copy()
        self.adaptation_rate = adaptation_rate

        # å†å²è®°å½•
        self.score_history = []
        self.param_history = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.improvement_count = 0
        self.stagnation_count = 0

        logger.info(f"âš™ï¸  è‡ªé€‚åº”å‚æ•°è°ƒæ•´å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   åˆå§‹å‚æ•°: {initial_params}")

    def adapt_mutation_rate(self, diversity: float, target_diversity: float = 0.5) -> float:
        """
        è‡ªé€‚åº”è°ƒæ•´å˜å¼‚ç‡

        Args:
            diversity: å½“å‰å¤šæ ·æ€§
            target_diversity: ç›®æ ‡å¤šæ ·æ€§

        Returns:
            è°ƒæ•´åçš„å˜å¼‚ç‡
        """
        current_rate = self.params.get('mutation_rate', 0.1)

        # å¦‚æœå¤šæ ·æ€§ä½ï¼Œå¢åŠ å˜å¼‚ç‡
        if diversity < target_diversity:
            new_rate = min(current_rate * (1 + self.adaptation_rate), 0.5)
        else:
            new_rate = max(current_rate * (1 - self.adaptation_rate), 0.01)

        self.params['mutation_rate'] = new_rate
        return new_rate

    def adapt_selection_pressure(self, convergence_rate: float) -> float:
        """
        è‡ªé€‚åº”è°ƒæ•´é€‰æ‹©å‹åŠ›

        Args:
            convergence_rate: æ”¶æ•›é€Ÿç‡

        Returns:
            è°ƒæ•´åçš„é€‰æ‹©å‹åŠ›
        """
        current_pressure = self.params.get('selection_pressure', 0.7)

        # å¦‚æœæ”¶æ•›å¤ªå¿«ï¼Œé™ä½é€‰æ‹©å‹åŠ›
        if convergence_rate > 0.9:
            new_pressure = max(current_pressure * (1 - self.adaptation_rate), 0.5)
        else:
            new_pressure = min(current_pressure * (1 + self.adaptation_rate), 0.95)

        self.params['selection_pressure'] = new_pressure
        return new_pressure

    def adapt_learning_rate(self, score_improvement: float) -> float:
        """
        è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡

        Args:
            score_improvement: åˆ†æ•°æ”¹å–„é‡

        Returns:
            è°ƒæ•´åçš„å­¦ä¹ ç‡
        """
        current_lr = self.params.get('learning_rate', 0.01)

        # å¦‚æœæ”¹å–„å°ï¼Œé™ä½å­¦ä¹ ç‡
        if score_improvement < 0.001:
            new_lr = max(current_lr * 0.9, 0.0001)
        else:
            new_lr = min(current_lr * 1.1, 0.1)

        self.params['learning_rate'] = new_lr
        return new_lr

    def update(self, score: float, diversity: float = None, **metrics):
        """
        æ›´æ–°å‚æ•°

        Args:
            score: å½“å‰åˆ†æ•°
            diversity: å¤šæ ·æ€§
            **metrics: å…¶ä»–æŒ‡æ ‡
        """
        self.score_history.append(score)
        self.param_history.append(self.params.copy())

        # è®¡ç®—æ”¹å–„
        if len(self.score_history) > 1:
            improvement = score - self.score_history[-2]

            if improvement > 0:
                self.improvement_count += 1
                self.stagnation_count = 0
            else:
                self.stagnation_count += 1

            # è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
            self.adapt_learning_rate(improvement)

        # è‡ªé€‚åº”è°ƒæ•´å˜å¼‚ç‡
        if diversity is not None:
            self.adapt_mutation_rate(diversity)

    def get_params(self) -> Dict[str, float]:
        """è·å–å½“å‰å‚æ•°"""
        return self.params.copy()


class BayesianOptimizer(BaseHyperparameterOptimizer):
    """
    è´å¶æ–¯ä¼˜åŒ–å™¨

    ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ä»£ç†æ¨¡å‹è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚

    æ ¸å¿ƒç‰¹æ€§:
    1. é«˜æ–¯è¿‡ç¨‹ä»£ç†æ¨¡å‹
    2. é‡‡é›†å‡½æ•°ä¼˜åŒ–ï¼ˆEIã€UCBï¼‰
    3. é«˜æ•ˆçš„å…¨å±€æœç´¢
    4. æ ·æœ¬æ•ˆç‡é«˜

    å‚è€ƒæ–‡çŒ®:
    Brochu, E., et al. (2010). A tutorial on Bayesian optimization of
    expensive cost functions, with application to active user modeling
    and hierarchical reinforcement learning.
    """

    def __init__(self,
                 hyperparameters: List[Hyperparameter],
                 objective_function: Callable[[Dict[str, Any]], float],
                 acquisition: str = 'ei',
                 n_warmup: int = 10000,
                 n_iter: int = 10):
        """
        åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–å™¨

        Args:
            hyperparameters: è¶…å‚æ•°åˆ—è¡¨
            objective_function: ç›®æ ‡å‡½æ•°
            acquisition: é‡‡é›†å‡½æ•° ('ei', 'ucb', 'pi')
            n_warmup: éšæœºçƒ­èº«é‡‡æ ·æ•°
            n_iter: é‡‡é›†å‡½æ•°ä¼˜åŒ–è¿­ä»£æ•°
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learn is required for Bayesian optimization")

        super().__init__(hyperparameters, objective_function)

        self.acquisition = acquisition
        self.n_warmup = n_warmup
        self.n_iter = n_iter

        # é«˜æ–¯è¿‡ç¨‹æ¨¡å‹
        kernel = ConstantKernel(1.0) * RBF(length_scale=1.0)
        self.gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)

        # è¯„ä¼°å†å²
        self.X = []
        self.y = []

        logger.info(f"ğŸ¯ è´å¶æ–¯ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   é‡‡é›†å‡½æ•°: {acquisition}")

    def _params_to_vector(self, params: Dict[str, Any]) -> np.ndarray:
        """å°†å‚æ•°å­—å…¸è½¬æ¢ä¸ºå‘é‡"""
        vector = []
        for hp in self.hyperparameters:
            value = params[hp.name]
            if hp.type == 'categorical':
                # One-hotç¼–ç 
                one_hot = [1.0 if v == value else 0.0 for v in hp.choices]
                vector.extend(one_hot)
            else:
                # å½’ä¸€åŒ–åˆ°[0, 1]
                normalized = (value - hp.min_val) / (hp.max_val - hp.min_val)
                vector.append(normalized)

        return np.array(vector)

    def _vector_to_params(self, vector: np.ndarray) -> Dict[str, Any]:
        """å°†å‘é‡è½¬æ¢ä¸ºå‚æ•°å­—å…¸"""
        params = {}
        idx = 0

        for hp in self.hyperparameters:
            if hp.type == 'categorical':
                # ä»one-hotè§£ç 
                one_hot = vector[idx:idx + len(hp.choices)]
                choice_idx = np.argmax(one_hot)
                params[hp.name] = hp.choices[choice_idx]
                idx += len(hp.choices)
            else:
                # ä»å½’ä¸€åŒ–å€¼è§£ç 
                normalized = vector[idx]
                value = hp.min_val + normalized * (hp.max_val - hp.min_val)

                # ç¦»æ•£åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if hp.type == 'discrete':
                    value = int(round(value))

                params[hp.name] = value
                idx += 1

        return params

    def _acquisition_function(self, X: np.ndarray) -> np.ndarray:
        """è®¡ç®—é‡‡é›†å‡½æ•°å€¼"""
        # é¢„æµ‹å‡å€¼å’Œæ ‡å‡†å·®
        y_mean, y_std = self.gp.predict(X, return_std=True)

        if self.acquisition == 'ei':
            # Expected Improvement
            y_best = np.max(self.y) if self.y else 0
            z = (y_mean - y_best) / (y_std + 1e-9)
            ei = (y_mean - y_best) * norm.cdf(z) + y_std * norm.pdf(z)
            return ei
        elif self.acquisition == 'ucb':
            # Upper Confidence Bound
            kappa = 2.576  # 99% confidence
            ucb = y_mean + kappa * y_std
            return ucb
        elif self.acquisition == 'pi':
            # Probability of Improvement
            y_best = np.max(self.y) if self.y else 0
            z = (y_mean - y_best - 0.01) / (y_std + 1e-9)
            pi = norm.cdf(z)
            return pi
        else:
            raise ValueError(f"Unknown acquisition: {self.acquisition}")

    def optimize(self, n_iterations: int = 100) -> Tuple[Dict[str, Any], float]:
        """
        ä¼˜åŒ–è¶…å‚æ•°

        Args:
            n_iterations: è¿­ä»£æ¬¡æ•°

        Returns:
            (best_params, best_score)
        """
        logger.info(f"ğŸ¯ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–ï¼Œè¿­ä»£æ¬¡æ•°: {n_iterations}")

        best_params = None
        best_score = -np.inf

        for iteration in range(n_iterations):
            # éšæœºçƒ­èº«æˆ–å»ºè®®é‡‡æ ·
            if len(self.y) < 5:  # å‰5æ¬¡éšæœºé‡‡æ ·
                params = {hp.name: hp.sample() for hp in self.hyperparameters}
            else:
                # æ‹ŸåˆGP
                X_array = np.array(self.X)
                y_array = np.array(self.y)
                self.gp.fit(X_array, y_array)

                # ä¼˜åŒ–é‡‡é›†å‡½æ•°
                def objective(x):
                    return -self._acquisition_function(x.reshape(1, -1))[0]

                bounds = [(0, 1)] * len(self._params_to_vector(
                    {hp.name: hp.default for hp in self.hyperparameters}
                ))

                # ç®€å•éšæœºæœç´¢ï¼ˆå®é™…åº”è¯¥ç”¨æ›´å¤æ‚çš„ä¼˜åŒ–å™¨ï¼‰
                best_x = None
                best_acq = -np.inf

                for _ in range(self.n_warmup):
                    x = np.random.rand(len(bounds))
                    acq_value = self._acquisition_function(x.reshape(1, -1))[0]

                    if acq_value > best_acq:
                        best_acq = acq_value
                        best_x = x

                # è½¬æ¢ä¸ºå‚æ•°
                params = self._vector_to_params(best_x)

            # è¯„ä¼°
            score = self.objective_function(params)

            # è®°å½•
            x_vector = self._params_to_vector(params)
            self.X.append(x_vector)
            self.y.append(score)
            self.history.append((params, score))

            # æ›´æ–°æœ€ä½³
            if score > best_score:
                best_score = score
                best_params = params

            # è¾“å‡ºè¿›åº¦
            if (iteration + 1) % 10 == 0:
                logger.info(
                    f"Iteration {iteration + 1}/{n_iterations} | "
                    f"Best: {best_score:.6f} | "
                    f"Current: {score:.6f}"
                )

        logger.info("âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆ")
        return best_params, best_score


def create_hyperparameter_optimizer(hyperparameters: List[Hyperparameter],
                                  objective_function: Callable[[Dict[str, Any]], float],
                                  method: str = 'bayesian',
                                  **kwargs) -> BaseHyperparameterOptimizer:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºè¶…å‚æ•°ä¼˜åŒ–å™¨

    Args:
        hyperparameters: è¶…å‚æ•°åˆ—è¡¨
        objective_function: ç›®æ ‡å‡½æ•°
        method: ä¼˜åŒ–æ–¹æ³• ('random', 'bayesian')
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        è¶…å‚æ•°ä¼˜åŒ–å™¨
    """
    if method == 'random':
        return RandomSearchOptimizer(hyperparameters, objective_function)
    elif method == 'bayesian':
        return BayesianOptimizer(hyperparameters, objective_function, **kwargs)
    else:
        raise ValueError(f"Unknown optimization method: {method}")


__all__ = [
    'Hyperparameter',
    'BaseHyperparameterOptimizer',
    'RandomSearchOptimizer',
    'AdaptiveParameterTuner',
    'BayesianOptimizer',
    'create_hyperparameter_optimizer',
]
