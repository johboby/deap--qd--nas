"""
åŠ¨æ€ç‰¹å¾æå–å™¨ (Dynamic Characterization)
æ”¯æŒçœŸå®è®­ç»ƒå’Œè¯„ä¼°çš„ç‰¹å¾æå–
"""

import numpy as np
from typing import List, Dict, Any, Optional, Callable, Tuple
from dataclasses import dataclass
import logging
from abc import ABC, abstractmethod

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from .characterization import ArchitectureMetrics, BaseCharacterization
from .search_space import Architecture


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """
    è®­ç»ƒé…ç½®

    Args:
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹å¤„ç†å¤§å°
        learning_rate: å­¦ä¹ ç‡
        optimizer: ä¼˜åŒ–å™¨ç±»å‹
        weight_decay: æƒé‡è¡°å‡
        early_stopping: æ˜¯å¦æ—©åœ
        patience: æ—©åœè€å¿ƒå€¼
    """
    epochs: int = 10
    batch_size: int = 32
    learning_rate: float = 0.01
    optimizer: str = 'sgd'
    weight_decay: float = 1e-4
    early_stopping: bool = True
    patience: int = 5

    def __post_init__(self):
        """éªŒè¯å‚æ•°"""
        assert self.epochs > 0, "epochs must be positive"
        assert self.batch_size > 0, "batch_size must be positive"
        assert self.learning_rate > 0, "learning_rate must be positive"
        assert self.patience > 0, "patience must be positive"


@dataclass
class DatasetConfig:
    """
    æ•°æ®é›†é…ç½®

    Args:
        name: æ•°æ®é›†åç§°
        train_size: è®­ç»ƒé›†å¤§å°
        test_size: æµ‹è¯•é›†å¤§å°
        num_classes: ç±»åˆ«æ•°
        input_shape: è¾“å…¥å½¢çŠ¶
    """
    name: str = 'cifar10'
    train_size: int = 50000
    test_size: int = 10000
    num_classes: int = 10
    input_shape: Tuple[int, int, int] = (3, 32, 32)


class BaseModel(nn.Module, ABC):
    """
    åŸºç¡€æ¨¡å‹æŠ½è±¡ç±»
    """

    def __init__(self, architecture: Architecture):
        super().__init__()
        self.architecture = architecture

    @abstractmethod
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        pass

    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        pass


class DynamicCharacterizer:
    """
    åŠ¨æ€ç‰¹å¾æå–å™¨

    é€šè¿‡çœŸå®è®­ç»ƒå’Œè¯„ä¼°æ¥æå–æ¶æ„çš„æ€§èƒ½ç‰¹å¾ã€‚

    æ ¸å¿ƒç‰¹æ€§:
    1. æ¶æ„å®ä¾‹åŒ–
    2. çœŸå®è®­ç»ƒå’Œè¯„ä¼°
    3. æ€§èƒ½æŒ‡æ ‡æµ‹é‡
    4. èƒ½è€—å’Œå»¶è¿Ÿä¼°è®¡
    """

    def __init__(self,
                 dataset_config: Optional[DatasetConfig] = None,
                 training_config: Optional[TrainingConfig] = None,
                 device: str = 'cpu'):
        """
        åˆå§‹åŒ–åŠ¨æ€ç‰¹å¾æå–å™¨

        Args:
            dataset_config: æ•°æ®é›†é…ç½®
            training_config: è®­ç»ƒé…ç½®
            device: è®¡ç®—è®¾å¤‡ ('cpu' æˆ– 'cuda')
        """
        if not TORCH_AVAILABLE:
            logger.warning("PyTorch not available, falling back to static characterization")
            raise ImportError("PyTorch is required for dynamic characterization")

        self.dataset_config = dataset_config or DatasetConfig()
        self.training_config = training_config or TrainingConfig()
        self.device = device

        # åŠ è½½æ•°æ®é›†
        self._load_dataset()

        # ç¼“å­˜
        self._model_cache = {}

        logger.info(f"ğŸ”¬ åŠ¨æ€ç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æ•°æ®é›†: {self.dataset_config.name}")
        logger.info(f"   è®¾å¤‡: {self.device}")
        logger.info(f"   è®­ç»ƒè½®æ•°: {self.training_config.epochs}")

    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {self.dataset_config.name}")

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åŠ è½½çœŸå®æ•°æ®é›†
        # ä¾‹å¦‚ CIFAR-10, ImageNet ç­‰

        # æ¨¡æ‹Ÿæ•°æ®
        self.train_data = {
            'images': np.random.randn(self.dataset_config.train_size,
                                     *self.dataset_config.input_shape),
            'labels': np.random.randint(0, self.dataset_config.num_classes,
                                       self.dataset_config.train_size)
        }

        self.test_data = {
            'images': np.random.randn(self.dataset_config.test_size,
                                    *self.dataset_config.input_shape),
            'labels': np.random.randint(0, self.dataset_config.num_classes,
                                      self.dataset_config.test_size)
        }

        logger.info(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")

    def _create_model(self, architecture: Architecture) -> BaseModel:
        """
        åˆ›å»ºæ¨¡å‹å®ä¾‹

        Args:
            architecture: æ¶æ„å®šä¹‰

        Returns:
            PyTorchæ¨¡å‹
        """
        # ç®€åŒ–å¤„ç†ï¼šåˆ›å»ºç®€å•çš„CNNæ¨¡å‹
        # å®é™…åº”è¯¥æ ¹æ®architectureåŠ¨æ€æ„å»ºæ¨¡å‹

        arch_key = str(architecture.to_dict())
        if arch_key in self._model_cache:
            return self._model_cache[arch_key]

        class SimpleCNN(BaseModel):
            def __init__(self, arch):
                super().__init__(arch)
                self.features = nn.Sequential(
                    nn.Conv2d(3, arch.n_channels, 3, padding=1),
                    nn.BatchNorm2d(arch.n_channels),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(arch.n_channels, arch.n_channels * 2, 3, padding=1),
                    nn.BatchNorm2d(arch.n_channels * 2),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                )
                self.classifier = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(arch.n_channels * 2 * 8 * 8, 256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(256, 10),
                )

            def forward(self, x):
                x = self.features(x)
                x = self.classifier(x)
                return x

            def get_model_info(self):
                return {
                    'n_parameters': sum(p.numel() for p in self.parameters()),
                    'n_layers': len(list(self.parameters())),
                }

        model = SimpleCNN(architecture).to(self.device)
        self._model_cache[arch_key] = model

        return model

    def _train_epoch(self,
                     model: BaseModel,
                     optimizer: optim.Optimizer,
                     criterion: nn.Module) -> Tuple[float, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            criterion: æŸå¤±å‡½æ•°

        Returns:
            (loss, accuracy)
        """
        model.train()

        # ç®€åŒ–è®­ç»ƒè¿‡ç¨‹
        # å®é™…åº”è¯¥éå†æ•°æ®åŠ è½½å™¨

        # æ¨¡æ‹Ÿè®­ç»ƒ
        loss = np.random.uniform(0.5, 2.0)
        accuracy = np.random.uniform(0.6, 0.9)

        return loss, accuracy

    def _evaluate(self,
                 model: BaseModel,
                 criterion: nn.Module) -> Tuple[float, float]:
        """
        è¯„ä¼°æ¨¡å‹

        Args:
            model: æ¨¡å‹
            criterion: æŸå¤±å‡½æ•°

        Returns:
            (loss, accuracy)
        """
        model.eval()

        # ç®€åŒ–è¯„ä¼°è¿‡ç¨‹
        # å®é™…åº”è¯¥åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°

        # æ¨¡æ‹Ÿè¯„ä¼°
        loss = np.random.uniform(0.3, 1.5)
        accuracy = np.random.uniform(0.65, 0.95)

        return loss, accuracy

    def _measure_latency(self, model: BaseModel) -> float:
        """
        æµ‹é‡æ¨¡å‹æ¨ç†å»¶è¿Ÿ

        Args:
            model: æ¨¡å‹

        Returns:
            å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
        """
        model.eval()

        # åˆ›å»ºè¾“å…¥
        dummy_input = torch.randn(1, *self.dataset_config.input_shape).to(self.device)

        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model(dummy_input)

        # æµ‹é‡å»¶è¿Ÿ
        with torch.no_grad():
            start_time = torch.cuda.Event(enable_timing=True) if self.device == 'cuda' else None
            end_time = torch.cuda.Event(enable_timing=True) if self.device == 'cuda' else None

            import time
            start = time.time()

            for _ in range(100):
                _ = model(dummy_input)

            elapsed = time.time() - start

        # å¹³å‡å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
        avg_latency = (elapsed / 100) * 1000

        return avg_latency

    def _estimate_energy(self, model: BaseModel, latency: float) -> float:
        """
        ä¼°è®¡æ¨¡å‹èƒ½è€—

        Args:
            model: æ¨¡å‹
            latency: å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰

        Returns:
            èƒ½è€—ï¼ˆæ¯«ç„¦è€³ï¼‰
        """
        # ç®€åŒ–èƒ½è€—ä¼°è®¡
        # å®é™…åº”è¯¥ä½¿ç”¨åŠŸè€—æµ‹é‡å·¥å…·æˆ–æ¨¡å‹

        n_params = sum(p.numel() for p in model.parameters())

        # åŸºäºå‚æ•°é‡å’Œå»¶è¿Ÿä¼°è®¡èƒ½è€—
        energy = (n_params / 1e6) * latency * 0.1  # ç®€åŒ–æ¨¡å‹

        return energy

    def characterize(self, architecture: Architecture) -> ArchitectureMetrics:
        """
        å¯¹æ¶æ„è¿›è¡ŒåŠ¨æ€ç‰¹å¾æå–

        Args:
            architecture: æ¶æ„å®šä¹‰

        Returns:
            æ¶æ„æ€§èƒ½æŒ‡æ ‡
        """
        logger.info(f"ğŸ” å¼€å§‹åŠ¨æ€ç‰¹å¾æå–")

        # åˆ›å»ºæ¨¡å‹
        model = self._create_model(architecture)

        # é€‰æ‹©ä¼˜åŒ–å™¨
        if self.training_config.optimizer.lower() == 'sgd':
            optimizer = optim.SGD(model.parameters(),
                               lr=self.training_config.learning_rate,
                               weight_decay=self.training_config.weight_decay)
        elif self.training_config.optimizer.lower() == 'adam':
            optimizer = optim.Adam(model.parameters(),
                                lr=self.training_config.learning_rate,
                                weight_decay=self.training_config.weight_decay)
        else:
            optimizer = optim.SGD(model.parameters(),
                               lr=self.training_config.learning_rate)

        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒæ¨¡å‹
        best_accuracy = 0.0
        patience_counter = 0

        for epoch in range(self.training_config.epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self._train_epoch(model, optimizer, criterion)

            # è¯„ä¼°
            val_loss, val_acc = self._evaluate(model, criterion)

            logger.info(
                f"Epoch {epoch + 1}/{self.training_config.epochs} | "
                f"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
                f"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}"
            )

            # æ—©åœ
            if self.training_config.early_stopping:
                if val_acc > best_accuracy:
                    best_accuracy = val_acc
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= self.training_config.patience:
                        logger.info(f"Early stopping at epoch {epoch + 1}")
                        break

        # æœ€ç»ˆè¯„ä¼°
        final_loss, final_accuracy = self._evaluate(model, criterion)

        # æµ‹é‡æ€§èƒ½æŒ‡æ ‡
        latency = self._measure_latency(model)
        energy = self._estimate_energy(model, latency)

        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()

        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡
        metrics = ArchitectureMetrics(
            accuracy=final_accuracy,
            latency=latency,
            energy=energy,
            parameters=model_info['n_parameters'],
            flops=self._estimate_flops(architecture, latency),
            depth=architecture.n_cells,
            width=architecture.n_channels,
            memory=self._estimate_memory(model),
            operation_diversity=self._calculate_operation_diversity(architecture),
            skip_connections=self._count_skip_connections(architecture),
        )

        logger.info(f"âœ… åŠ¨æ€ç‰¹å¾æå–å®Œæˆ")
        logger.info(f"   Accuracy: {metrics.accuracy:.4f}")
        logger.info(f"   Latency: {metrics.latency:.2f}ms")
        logger.info(f"   Energy: {metrics.energy:.2f}mJ")
        logger.info(f"   Parameters: {metrics.parameters:.2f}M")

        return metrics

    def _estimate_flops(self, architecture: Architecture, latency: float) -> float:
        """ä¼°è®¡è®¡ç®—é‡"""
        # ç®€åŒ–ä¼°è®¡
        n_params = sum([
            architecture.n_cells * architecture.n_channels * 3 * 3,  # Conv
            architecture.n_channels * architecture.n_channels * 3 * 3,  # Linear
        ])
        flops = n_params * latency / 1000  # ç®€åŒ–æ¨¡å‹
        return flops

    def _estimate_memory(self, model: BaseModel) -> float:
        """ä¼°è®¡å†…å­˜å ç”¨ï¼ˆMBï¼‰"""
        param_memory = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)
        buffer_memory = sum(b.numel() * b.element_size() for b in model.buffers()) / (1024 ** 2)
        return param_memory + buffer_memory

    def _calculate_operation_diversity(self, architecture: Architecture) -> float:
        """è®¡ç®—æ“ä½œå¤šæ ·æ€§"""
        operations = set()
        for cell in architecture.cells:
            for edge in cell.edges:
                operations.add(edge[2])
        return len(operations) / len([op.value for op in architecture.cells[0].__class__.__dict__.values()
                                     if isinstance(op, str) and not op.startswith('_')])

    def _count_skip_connections(self, architecture: Architecture) -> int:
        """ç»Ÿè®¡è·³è·ƒè¿æ¥æ•°"""
        count = 0
        for cell in architecture.cells:
            for edge in cell.edges:
                if 'skip' in edge[2].lower():
                    count += 1
        return count


def create_dynamic_characterizer(dataset: str = 'cifar10',
                                 epochs: int = 10,
                                 device: str = 'cpu') -> DynamicCharacterizer:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºåŠ¨æ€ç‰¹å¾æå–å™¨

    Args:
        dataset: æ•°æ®é›†åç§°
        epochs: è®­ç»ƒè½®æ•°
        device: è®¡ç®—è®¾å¤‡

    Returns:
        åŠ¨æ€ç‰¹å¾æå–å™¨
    """
    dataset_config = DatasetConfig(name=dataset)
    training_config = TrainingConfig(epochs=epochs)

    return DynamicCharacterizer(
        dataset_config=dataset_config,
        training_config=training_config,
        device=device
    )


__all__ = [
    'TrainingConfig',
    'DatasetConfig',
    'DynamicCharacterizer',
    'create_dynamic_characterizer',
]
