"""
NASåŸºå‡†æµ‹è¯•å¥—ä»¶ (NAS Benchmark Suite)
é›†æˆæ ‡å‡†NASåŸºå‡†å’Œæ•°æ®é›†
"""

import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging

try:
    import torch
    import torch.nn as nn
    import torchvision
    import torchvision.transforms as transforms
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

from .search_space import Architecture, SearchSpace
from .characterization import ArchitectureMetrics


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DatasetConfig:
    """
    æ•°æ®é›†é…ç½®

    Args:
        name: æ•°æ®é›†åç§°
        num_classes: ç±»åˆ«æ•°
        input_shape: è¾“å…¥å½¢çŠ¶
        train_size: è®­ç»ƒé›†å¤§å°
        test_size: æµ‹è¯•é›†å¤§å°
        mean: æ•°æ®å‡å€¼
        std: æ•°æ®æ ‡å‡†å·®
    """
    name: str
    num_classes: int
    input_shape: Tuple[int, int, int]  # (C, H, W)
    train_size: int
    test_size: int
    mean: Tuple[float, float, float]
    std: Tuple[float, float, float]

    def __str__(self) -> str:
        return self.name


class StandardDatasets:
    """æ ‡å‡†æ•°æ®é›†"""

    @staticmethod
    def get_cifar10() -> DatasetConfig:
        """CIFAR-10"""
        return DatasetConfig(
            name='cifar10',
            num_classes=10,
            input_shape=(3, 32, 32),
            train_size=50000,
            test_size=10000,
            mean=(0.4914, 0.4822, 0.4465),
            std=(0.2023, 0.1994, 0.2010),
        )

    @staticmethod
    def get_cifar100() -> DatasetConfig:
        """CIFAR-100"""
        return DatasetConfig(
            name='cifar100',
            num_classes=100,
            input_shape=(3, 32, 32),
            train_size=50000,
            test_size=10000,
            mean=(0.5071, 0.4865, 0.4409),
            std=(0.2009, 0.1984, 0.2023),
        )

    @staticmethod
    def get_mnist() -> DatasetConfig:
        """MNIST"""
        return DatasetConfig(
            name='mnist',
            num_classes=10,
            input_shape=(1, 28, 28),
            train_size=60000,
            test_size=10000,
            mean=(0.1307,),
            std=(0.3081,),
        )

    @staticmethod
    def get_imagenet() -> DatasetConfig:
        """ImageNetï¼ˆç®€åŒ–é…ç½®ï¼‰"""
        return DatasetConfig(
            name='imagenet',
            num_classes=1000,
            input_shape=(3, 224, 224),
            train_size=1281167,
            test_size=50000,
            mean=(0.485, 0.456, 0.406),
            std=(0.229, 0.224, 0.225),
        )


class BaseNASBenchmark(ABC):
    """
    NASåŸºå‡†æµ‹è¯•åŸºç±»
    """

    def __init__(self,
                 dataset_config: DatasetConfig,
                 search_space: SearchSpace,
                 device: str = 'cpu'):
        """
        åˆå§‹åŒ–NASåŸºå‡†

        Args:
            dataset_config: æ•°æ®é›†é…ç½®
            search_space: æœç´¢ç©ºé—´
            device: è®¡ç®—è®¾å¤‡
        """
        self.dataset_config = dataset_config
        self.search_space = search_space
        self.device = device

        # åŠ è½½æ•°æ®é›†
        self._load_dataset()

        logger.info(f"ğŸ“Š NASåŸºå‡†åˆå§‹åŒ–å®Œæˆ: {dataset_config.name}")

    @abstractmethod
    def _load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        pass

    @abstractmethod
    def train_model(self, architecture: Architecture, epochs: int) -> Tuple[float, float]:
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            architecture: æ¶æ„
            epochs: è®­ç»ƒè½®æ•°

        Returns:
            (train_accuracy, test_accuracy)
        """
        pass

    @abstractmethod
    def evaluate_architecture(self, architecture: Architecture) -> ArchitectureMetrics:
        """
        è¯„ä¼°æ¶æ„

        Args:
            architecture: æ¶æ„

        Returns:
            æ€§èƒ½æŒ‡æ ‡
        """
        pass


class CIFAR10Benchmark(BaseNASBenchmark):
    """
    CIFAR-10 NASåŸºå‡†

    æ ‡å‡†çš„CIFAR-10 NASåŸºå‡†æµ‹è¯•ã€‚
    """

    def __init__(self,
                 search_space: SearchSpace,
                 data_dir: str = './data',
                 device: str = 'cpu',
                 batch_size: int = 128):
        """
        åˆå§‹åŒ–CIFAR-10åŸºå‡†

        Args:
            search_space: æœç´¢ç©ºé—´
            data_dir: æ•°æ®ç›®å½•
            device: è®¡ç®—è®¾å¤‡
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.data_dir = data_dir
        self.batch_size = batch_size

        dataset_config = StandardDatasets.get_cifar10()

        super().__init__(dataset_config, search_space, device)

    def _load_dataset(self):
        """åŠ è½½CIFAR-10æ•°æ®é›†"""
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch is required for CIFAR-10 benchmark")

        logger.info("ğŸ“¥ åŠ è½½CIFAR-10æ•°æ®é›†")

        # æ•°æ®å¢å¼º
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(self.dataset_config.mean, self.dataset_config.std),
        ])

        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(self.dataset_config.mean, self.dataset_config.std),
        ])

        # åŠ è½½æ•°æ®é›†
        self.trainset = torchvision.datasets.CIFAR10(
            root=self.data_dir,
            train=True,
            download=True,
            transform=transform_train
        )

        self.testset = torchvision.datasets.CIFAR10(
            root=self.data_dir,
            train=False,
            download=True,
            transform=transform_test
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.trainloader = torch.utils.data.DataLoader(
            self.trainset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=2
        )

        self.testloader = torch.utils.data.DataLoader(
            self.testset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=2
        )

        logger.info(f"âœ… CIFAR-10æ•°æ®é›†åŠ è½½å®Œæˆ")
        logger.info(f"   è®­ç»ƒé›†: {len(self.trainset)}")
        logger.info(f"   æµ‹è¯•é›†: {len(self.testset)}")

    def train_model(self, architecture: Architecture, epochs: int = 50) -> Tuple[float, float]:
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            architecture: æ¶æ„
            epochs: è®­ç»ƒè½®æ•°

        Returns:
            (train_accuracy, test_accuracy)
        """
        # åˆ›å»ºæ¨¡å‹
        model = self._create_model(architecture)

        # è®­ç»ƒ
        train_acc = self._train(model, epochs)

        # æµ‹è¯•
        test_acc = self._test(model)

        return train_acc, test_acc

    def _create_model(self, architecture: Architecture) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        class SimpleCNN(nn.Module):
            def __init__(self, arch):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, arch.n_channels, 3, padding=1),
                    nn.BatchNorm2d(arch.n_channels),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                    nn.Conv2d(arch.n_channels, arch.n_channels * 2, 3, padding=1),
                    nn.BatchNorm2d(arch.n_channels * 2),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2),
                )
                self.classifier = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(arch.n_channels * 2 * 8 * 8, 256),
                    nn.ReLU(inplace=True),
                    nn.Dropout(0.5),
                    nn.Linear(256, 10),
                )

            def forward(self, x):
                x = self.features(x)
                x = self.classifier(x)
                return x

        model = SimpleCNN(architecture).to(self.device)
        return model

    def _train(self, model: nn.Module, epochs: int) -> float:
        """è®­ç»ƒæ¨¡å‹"""
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)

        model.train()
        correct = 0
        total = 0

        for epoch in range(epochs):
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

            scheduler.step()

        accuracy = 100. * correct / total
        return accuracy

    def _test(self, model: nn.Module) -> float:
        """æµ‹è¯•æ¨¡å‹"""
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in self.testloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                outputs = model(inputs)

                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        accuracy = 100. * correct / total
        return accuracy

    def evaluate_architecture(self, architecture: Architecture) -> ArchitectureMetrics:
        """
        è¯„ä¼°æ¶æ„

        Args:
            architecture: æ¶æ„

        Returns:
            æ€§èƒ½æŒ‡æ ‡
        """
        logger.info(f"ğŸ” è¯„ä¼°æ¶æ„: {architecture.to_dict()}")

        # è®­ç»ƒ
        train_acc, test_acc = self.train_model(architecture, epochs=50)

        # ä¼°è®¡å»¶è¿Ÿå’Œèƒ½è€—ï¼ˆç®€åŒ–ï¼‰
        n_params = sum(p.numel() for p in self._create_model(architecture).parameters())
        latency = n_params / 1e6 * 10  # ç®€åŒ–ä¼°è®¡
        energy = n_params / 1e6 * 5  # ç®€åŒ–ä¼°è®¡

        metrics = ArchitectureMetrics(
            accuracy=test_acc / 100,
            latency=latency,
            energy=energy,
            parameters=n_params,
            flops=n_params * 10,  # ç®€åŒ–ä¼°è®¡
            depth=architecture.n_cells,
            width=architecture.n_channels,
            memory=n_params * 4 / (1024 ** 2),  # å‡è®¾float32
            operation_diversity=0.8,
            skip_connections=0,
        )

        logger.info(f"âœ… è¯„ä¼°å®Œæˆ: Accuracy={test_acc:.2f}%")

        return metrics


class BenchmarkResults:
    """
    åŸºå‡†æµ‹è¯•ç»“æœ

    å­˜å‚¨å’Œæ¯”è¾ƒä¸åŒNASæ–¹æ³•çš„æ€§èƒ½ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•ç»“æœ"""
        self.results: List[Dict[str, Any]] = []

    def add_result(self,
                  method_name: str,
                  architecture: Architecture,
                  metrics: ArchitectureMetrics,
                  runtime: float):
        """
        æ·»åŠ ç»“æœ

        Args:
            method_name: æ–¹æ³•åç§°
            architecture: æ¶æ„
            metrics: æ€§èƒ½æŒ‡æ ‡
            runtime: è¿è¡Œæ—¶é—´
        """
        result = {
            'method': method_name,
            'architecture': architecture.to_dict(),
            'accuracy': metrics.accuracy,
            'latency': metrics.latency,
            'energy': metrics.energy,
            'parameters': metrics.parameters,
            'runtime': runtime,
        }
        self.results.append(result)

    def get_comparison(self) -> pd.DataFrame:
        """
        è·å–æ¯”è¾ƒè¡¨æ ¼

        Returns:
            æ¯”è¾ƒDataFrame
        """
        try:
            import pandas as pd
            df = pd.DataFrame(self.results)
            return df
        except ImportError:
            logger.warning("Pandas not available, cannot create DataFrame")
            return None

    def save_results(self, filepath: str):
        """
        ä¿å­˜ç»“æœ

        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        import json

        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2)

        logger.info(f"ç»“æœä¿å­˜è‡³: {filepath}")

    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print("åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)

        if not self.results:
            print("æ— ç»“æœ")
            return

        # æŒ‰æ–¹æ³•åˆ†ç»„
        methods = {}
        for result in self.results:
            method = result['method']
            if method not in methods:
                methods[method] = []
            methods[method].append(result)

        # æ‰“å°æ¯ä¸ªæ–¹æ³•çš„ç»Ÿè®¡
        for method, method_results in methods.items():
            accuracies = [r['accuracy'] for r in method_results]
            latencies = [r['latency'] for r in method_results]
            parameters = [r['parameters'] for r in method_results]

            print(f"\n{method}:")
            print(f"  å¹³å‡å‡†ç¡®ç‡: {np.mean(accuracies):.4f}")
            print(f"  å¹³å‡å»¶è¿Ÿ: {np.mean(latencies):.2f}ms")
            print(f"  å¹³å‡å‚æ•°é‡: {np.mean(parameters)/1e6:.2f}M")

        print("\n" + "="*60)


class BenchmarkRunner:
    """
    åŸºå‡†æµ‹è¯•è¿è¡Œå™¨

    è¿è¡Œå’Œæ¯”è¾ƒä¸åŒçš„NASæ–¹æ³•ã€‚
    """

    def __init__(self,
                 benchmark: BaseNASBenchmark,
                 search_space: SearchSpace):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•è¿è¡Œå™¨

        Args:
            benchmark: åŸºå‡†æµ‹è¯•
            search_space: æœç´¢ç©ºé—´
        """
        self.benchmark = benchmark
        self.search_space = search_space
        self.results = BenchmarkResults()

    def run_benchmark(self,
                     method_name: str,
                     n_samples: int = 10) -> Dict[str, Any]:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•

        Args:
            method_name: æ–¹æ³•åç§°
            n_samples: é‡‡æ ·æ•°é‡

        Returns:
            ç»Ÿè®¡ç»“æœ
        """
        logger.info(f"ğŸƒ è¿è¡ŒåŸºå‡†æµ‹è¯•: {method_name}")

        import time
        start_time = time.time()

        # é‡‡æ ·æ¶æ„å¹¶è¯„ä¼°
        architectures = [self.search_space.random_sample() for _ in range(n_samples)]

        for arch in architectures:
            metrics = self.benchmark.evaluate_architecture(arch)
            self.results.add_result(method_name, arch, metrics, time.time() - start_time)

        # è®¡ç®—ç»Ÿè®¡
        method_results = [r for r in self.results.results if r['method'] == method_name]

        stats = {
            'method': method_name,
            'mean_accuracy': np.mean([r['accuracy'] for r in method_results]),
            'std_accuracy': np.std([r['accuracy'] for r in method_results]),
            'mean_latency': np.mean([r['latency'] for r in method_results]),
            'mean_parameters': np.mean([r['parameters'] for r in method_results]),
        }

        logger.info(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ: {method_name}")
        return stats


def create_benchmark(dataset_name: str = 'cifar10',
                   search_space: Optional[SearchSpace] = None,
                   **kwargs) -> BaseNASBenchmark:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºåŸºå‡†æµ‹è¯•

    Args:
        dataset_name: æ•°æ®é›†åç§°
        search_space: æœç´¢ç©ºé—´
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        åŸºå‡†æµ‹è¯•å¯¹è±¡
    """
    search_space = search_space or SearchSpace()

    if dataset_name.lower() == 'cifar10':
        return CIFAR10Benchmark(search_space=search_space, **kwargs)
    elif dataset_name.lower() == 'cifar100':
        return CIFAR10Benchmark(search_space=search_space, **kwargs)
    elif dataset_name.lower() == 'mnist':
        return CIFAR10Benchmark(search_space=search_space, **kwargs)
    else:
        raise ValueError(f"Unknown dataset: {dataset_name}")


__all__ = [
    'DatasetConfig',
    'StandardDatasets',
    'BaseNASBenchmark',
    'CIFAR10Benchmark',
    'BenchmarkResults',
    'BenchmarkRunner',
    'create_benchmark',
]
